import os
import duckdb
import chromadb
import logging
import re
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings, FakeEmbeddings
import pandas as pd
from pathlib import Path
import json
import re
import uuid
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from typing import Optional, Dict, Any, List

# Import execute_sql_query from response_generator 
from response_generator import execute_sql_query # This is fine if response_generator does not import rag_core

# Configure logging
logger = logging.getLogger(__name__)

# Add spaCy for NER
try:
    import spacy # type: ignore # Imported conditionally - may not be installed
    try:
        nlp = spacy.load("en_core_web_sm")
        spacy_available = True
        logger.info("spaCy NER model loaded successfully")
    except Exception as e:
        spacy_available = False
        logger.warning(f"Could not load spaCy model 'en_core_web_sm': {e}. NER features might be limited.")
except ImportError:
    spacy_available = False
    logger.warning("spaCy not available, NER features will be disabled")

# Path to Chroma DB
CHROMA_DB_PATH = os.environ.get("CHROMA_DB_PATH", "./chroma_db")
logger.info(f"Using Chroma DB path: {CHROMA_DB_PATH}")

# Initialize embeddings model
try:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        logger.warning("GOOGLE_API_KEY environment variable not set. Using local HuggingFace embeddings.")
        try:
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.info("Using HuggingFace embeddings model (all-MiniLM-L6-v2)")
        except Exception as hf_error:
            logger.error(f"Error initializing HuggingFace embeddings: {hf_error}. Using FakeEmbeddings.")
            embeddings = FakeEmbeddings(size=384) # all-MiniLM-L6-v2 dim is 384
    else:
        try:
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
            genai.configure(api_key=api_key) # type: ignore # genai from main.py import
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
            logger.info("Using Google Generative AI Embeddings model (models/embedding-001)")
        except Exception as google_error:
            logger.error(f"Error initializing Google embeddings: {google_error}. Falling back to HuggingFace.")
            try:
                embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                logger.info("Using HuggingFace embeddings model as fallback.")
            except Exception as hf_fallback_error:
                logger.error(f"Error initializing HuggingFace fallback embeddings: {hf_fallback_error}. Using FakeEmbeddings.")
                embeddings = FakeEmbeddings(size=384)
except Exception as e_init_error:
    logger.error(f"Critical error initializing embeddings model: {e_init_error}. Using FakeEmbeddings.")
    embeddings = FakeEmbeddings(size=384)


# Initialize Chroma client - lazily in get_chroma_collection()
chroma_client: Optional[chromadb.PersistentClient] = None
collection_name = "docuquery"

# Initialize DuckDB connection - THIS IS THE SHARED CONNECTION
try:
    # Using duckdb.connect(':memory:') creates/connects to the default in-memory database.
    # Subsequent calls to duckdb.connect(':memory:') in the same process will connect to the *same* database.
    db_conn = duckdb.connect(database=":memory:", read_only=False)
    logger.info("RAG Core: Shared DuckDB connection established to :memory:")
except Exception as e:
    logger.error(f"RAG Core: Error connecting to shared DuckDB: {e}")
    raise # Critical if DB can't be initialized

def get_chroma_collection():
    """Get or create Chroma collection, ensuring client is initialized."""
    global chroma_client
    try:
        if chroma_client is None:
            os.makedirs(CHROMA_DB_PATH, exist_ok=True)
            logger.info(f"Initializing Chroma client with path: {CHROMA_DB_PATH}")
            # Added a setting to allow schema upgrades or handle inconsistencies, common with Chroma versions/restarts.
            # For a persistent client, this can help if the DB exists but has issues.
            # Note: chromadb.PersistentClient does not directly take http client settings like this.
            # The Settings object is for HttpClient. For PersistentClient, it's simpler.
            try:
                chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
                logger.info("Chroma client initialized successfully.")
            except Exception as client_init_error:
                logger.error(f"Error initializing Chroma PersistentClient: {client_init_error}")
                # Attempt to clear and reinitialize if specific errors occur (e.g. locked DB, corruption)
                # This is a drastic measure and should be used cautiously.
                # For now, we'll just re-raise or return None.
                return None
            
        # Get or create collection using the initialized client
        # The Langchain Chroma wrapper handles the embedding function association.
        langchain_chroma_collection = Chroma(
                client=chroma_client,
                collection_name=collection_name,
                embedding_function=embeddings
            )
        logger.info(f"Chroma collection '{collection_name}' accessed/created via Langchain wrapper.")
        return langchain_chroma_collection
    
    except Exception as e:
        logger.error(f"Error getting Chroma collection: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def extract_entities_and_sections(text: str, metadata: Optional[Dict[str, Any]] = None) -> tuple[Dict[str, List[str]], Dict[str, str]]:
    """Extract named entities (PERSON) and potential sections from text."""
    entities: Dict[str, List[str]] = {"PERSON": []}
    sections: Dict[str, str] = {} # section_name: type (e.g. "markdown", "uppercase")
    
    if not text: return entities, sections

    # Regex for sections (improved robustness)
    section_patterns = [
        (r"^\s*#{1,6}\s+(.+?)\s*#*\s*$", "markdown"),  # Markdown headers (match full line)
        (r"^\s*([A-Z0-9][A-Z0-9\s]{3,48}[A-Z0-9])\s*(?::|\n)", "uppercase"), # All caps (4-50 chars), ends with : or newline
        (r"^\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,5})\s*:", "title_case_colon") # Title Case up to 6 words, ends with :
    ]
    for line in text.splitlines(): # Process line by line for section headers
        for pattern, p_type in section_patterns:
            match = re.match(pattern, line.strip())
            if match:
                section_name = match.group(1).strip()
                if section_name and 2 < len(section_name) < 100: # Basic sanity check
                    sections[section_name] = p_type
                    break # Found a section header on this line

    if not spacy_available:
        # Basic regex for PERSON if spaCy not available (primarily for resumes)
        if metadata and ("resume" in metadata.get("source", "").lower() or "cv" in metadata.get("source", "").lower()):
            # Try to find name-like patterns at the beginning of the text
            for line in text.split('\n')[:5]: # Check first 5 lines
                # Pattern for "First Last" or "First M. Last"
                name_match = re.match(r"^\s*([A-Z][a-z'-]+(?:\s+[A-Z][a-z'-]*\.?){1,3})\s*$", line.strip())
                if name_match:
                    potential_name = name_match.group(1).strip()
                    if len(potential_name.split()) >= 2: # At least two parts to the name
                        entities["PERSON"].append(potential_name)
                        break # Found a name
        return entities, sections

    try:
        doc_spacy = nlp(text)
        for ent in doc_spacy.ents:
            if ent.label_ == "PERSON" and ent.text.strip() not in entities["PERSON"]:
                # Filter out very short or generic "PERSON" entities if needed
                if len(ent.text.strip().split()) > 1 and len(ent.text.strip()) > 3: # e.g. "John Doe", not "X" or "Dr"
                     entities["PERSON"].append(ent.text.strip())
    except Exception as e_spacy:
        logger.warning(f"Error during spaCy entity extraction: {e_spacy}")
        
    return entities, sections


def register_duckdb_table(df: pd.DataFrame, table_name_base: str) -> Optional[str]:
    """Register a pandas DataFrame as a DuckDB table using the shared db_conn."""
    global db_conn # Use the shared connection from rag_core
    try:
        # Sanitize table name: ensure it's a valid SQL identifier
        # Replace non-alphanumeric with underscore, ensure starts with letter or underscore
        sanitized_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name_base)
        if not re.match(r'^[a-zA-Z_]', sanitized_name): # Must start with letter or underscore
            sanitized_name = "_" + sanitized_name
        
        # Ensure name is not a SQL keyword (basic check)
        if sanitized_name.upper() in ["SELECT", "TABLE", "FROM", "WHERE"]: # Add more if needed
            sanitized_name += "_data"

        # Handle non-serializable data types for DuckDB compatibility if any remain
        for col in df.columns:
            # Convert object columns that might contain problematic types (e.g. complex objects) to string
            if df[col].dtype.name == 'object':
                try:
                    # A quick check if it's mostly simple types; this isn't foolproof
                    if any(isinstance(x, (list, dict, set)) for x in df[col].dropna().head()):
                        df[col] = df[col].astype(str)
                except Exception: # Broad except if introspection fails
                     df[col] = df[col].astype(str)
            # Timestamps with timezone can be problematic; DuckDB prefers naive or UTC
            if pd.api.types.is_datetime64_any_dtype(df[col]) and getattr(df[col].dt, 'tz', None) is not None:
                try:
                    df[col] = df[col].dt.tz_convert(None) # Convert to timezone-naive
                    logger.info(f"Converted timezone-aware column '{col}' to naive for DuckDB.")
                except Exception as tz_err:
                    logger.warning(f"Could not convert timezone for column '{col}': {tz_err}. Converting to string.")
                    df[col] = df[col].astype(str)
        
        db_conn.register(sanitized_name, df)
        # Verify registration
        result = db_conn.execute(f"SELECT COUNT(*) FROM \"{sanitized_name}\"").fetchone()
        if result is None: # Should not happen if register didn't throw error
            logger.error(f"Failed to verify table registration for '{sanitized_name}' (count returned None)")
            return None
        logger.info(f"Successfully registered table '{sanitized_name}' with {result[0]} rows in shared DuckDB.")
        return sanitized_name
            
    except Exception as e:
        logger.error(f"Error registering DuckDB table '{table_name_base}' (sanitized to '{sanitized_name if 'sanitized_name' in locals() else ''}'): {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def embed(documents: List[Document], file_id: str) -> int:
    """
    Process documents (enrich metadata) and add them to Chroma vector store.
    Assumes documents have 'source' and 'file_type' in metadata from load_xxx functions.
    """
    try:
        logger.info(f"Embedding: Processing {len(documents)} documents for file_id: {file_id}")
        if not documents:
            logger.warning(f"Embedding: No documents provided for file_id: {file_id}")
            return 0
            
        total_chunks_embedded = 0
        valid_documents_for_chroma = []
        
        for i, doc in enumerate(documents):
            if not hasattr(doc, 'page_content') or not doc.page_content or not doc.page_content.strip():
                doc_source = doc.metadata.get('source', 'unknown_source') if hasattr(doc, 'metadata') and doc.metadata else 'unknown_source_no_meta'
                logger.warning(f"Embedding: Document {i} from '{doc_source}' (file_id: {file_id}) has no content, skipping.")
                continue
                
            if not hasattr(doc, 'metadata') or doc.metadata is None:
                doc.metadata = {} # Initialize metadata if missing
            
            # Ensure file_id is consistently set
            doc.metadata['file_id'] = file_id
            
            # Enrich with entities and sections (persons, sections)
            # extract_entities_and_sections expects metadata for context (e.g. filename for resume name extraction)
            entities, sections_dict = extract_entities_and_sections(doc.page_content, doc.metadata)
            doc.metadata['persons'] = entities.get("PERSON", []) # List of person names
            doc.metadata['sections'] = list(sections_dict.keys()) # List of section names

            # Sanitize metadata for Chroma: values must be str, int, float, bool, or None
            # ChromaDB has specific limitations on metadata types.
            sanitized_meta = {}
            for k, v in doc.metadata.items():
                if isinstance(v, (str, int, float, bool)) or v is None:
                    sanitized_meta[k] = v
                elif isinstance(v, list): # Allow lists of simple types
                    sanitized_meta[k] = [str(item) if not isinstance(item, (str, int, float, bool)) else item for item in v]
                else: # Convert other types to string
                    sanitized_meta[k] = str(v)
            doc.metadata = sanitized_meta
            
            valid_documents_for_chroma.append(doc)
        
        if not valid_documents_for_chroma:
            logger.warning(f"Embedding: No valid documents to add to Chroma for file_id: {file_id}")
            return 0

        collection = get_chroma_collection()
        if collection is None:
            logger.error(f"Embedding: Failed to get Chroma collection for file_id: {file_id}. Documents not embedded.")
            return 0
        
        # Add documents to Chroma in batches if necessary (though Langchain wrapper might handle this)
        # For large number of docs, consider batching:
        # batch_size = 100 
        # for i in range(0, len(valid_documents_for_chroma), batch_size):
        #    batch = valid_documents_for_chroma[i:i + batch_size]
        #    collection.add_documents(batch)
        #    total_chunks_embedded += len(batch)
        collection.add_documents(valid_documents_for_chroma)
        total_chunks_embedded = len(valid_documents_for_chroma)
        logger.info(f"Embedding: Successfully added {total_chunks_embedded} chunks to Chroma for file_id: {file_id}")
        
        return total_chunks_embedded
    
    except Exception as e:
        logger.error(f"Error in embed function for file_id {file_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 0


def original_retrieve(query: str, k=5, metadata_filter=None):
    """Original retrieve function - retrieves relevant documents or execute SQL query"""
    try:
        logger.info(f"Retrieval: Processing query: '{query}' with filter: {metadata_filter}")
        
        # Direct SQL execution if query looks like SQL
        # A more robust check might involve a simple SQL parser or more keywords.
        if query.strip().lower().startswith("select") and " from " in query.lower():
            logger.info("Retrieval: Query identified as SQL, executing directly.")
            return execute_sql_query(query) # Uses response_generator's execute_sql_query
        
        collection = get_chroma_collection()
        if collection is None:
            logger.error("Retrieval: Failed to get Chroma collection.")
            return {"type": "error", "message": "Failed to access vector database"}
        
        # Similarity search
        # The filter in ChromaDB should be a dict e.g. {"file_id": "some_id"}
        docs = collection.similarity_search(query=query, k=k, filter=metadata_filter)
        logger.info(f"Retrieval: Similarity search returned {len(docs)} documents.")
        
        # (Optional) Add hints about SQL if tabular data is found (already handled well in main.py's chat endpoint)
        # For example, if docs contain metadata indicating they are from tables.
        
        return {"type": "documents", "data": docs}
        
    except Exception as e:
        logger.error(f"Error in original_retrieve function: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"type": "error", "message": f"Error retrieving results: {str(e)}"}

def retrieve_with_compression(query: str, metadata_filter: Optional[Dict[str, Any]] = None, k: int = 5) -> Dict[str, Any]:
    """Enhanced retrieval with contextual compression."""
    try:
        # Get more documents initially for better compression context
        initial_k = k * 2 
        result = original_retrieve(query, k=initial_k, metadata_filter=metadata_filter)
        
        if result["type"] != "documents" or not result.get("data"):
            logger.info("Compression: No documents from original retrieval or error. Skipping compression.")
            return result 
            
        docs = result["data"]
        logger.info(f"Compression: Retrieved {len(docs)} documents for potential compression.")
        
        # Initialize LLM for compression
        comp_llm = None
        if os.getenv("GOOGLE_API_KEY"):
            from langchain_google_genai import ChatGoogleGenerativeAI
            comp_llm = ChatGoogleGenerativeAI(
                model="gemini-pro", # Using gemini-pro for compression tasks
                temperature=0, # Low temperature for factual extraction
                google_api_key=os.getenv("GOOGLE_API_KEY")
            )
            logger.info("Compression: Using Google Gemini-Pro for LLMChainExtractor.")
        else:
            # Fallback for compression if no API key (might be less effective)
            # A simple keyword-based filter could be an alternative here.
            # For now, FakeListLLM means compression might not change much.
            from langchain_core.language_models.fake import FakeListLLM
            comp_llm = FakeListLLM(responses=["No specific relevant context found."]) # More neutral fake response
            logger.warning("Compression: No Google API key. LLMChainExtractor using FakeListLLM; compression may be limited.")
        
        compressor = LLMChainExtractor.from_llm(comp_llm)
        
        # Base retriever from the initially retrieved documents
        from langchain.retrievers import ContextualCompressionRetriever, DocumentCompressorPipeline
        from langchain_community.document_transformers import EmbeddingsRedundantFilter
        from langchain.retrievers.document_compressors import DocumentCompressorPipeline
        
        # Simple retriever from the list of docs
        from langchain.schema import BaseRetriever
        class ListRetriever(BaseRetriever):
            documents: List[Document]
            def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
                return self.documents
            async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
                return self.documents

        base_retriever = ListRetriever(documents=docs)

        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        
        compressed_docs = compression_retriever.get_relevant_documents(query)
        logger.info(f"Compression: Retrieved {len(compressed_docs)} compressed documents.")
        
        final_docs = compressed_docs if compressed_docs else docs # Fallback if compression yields nothing
        return {"type": "documents", "data": final_docs[:k]} # Return up to k documents
            
    except Exception as compression_error:
        logger.warning(f"Error during document compression: {compression_error}. Falling back to standard retrieval.")
        # Fallback to original retrieve without compression if compression itself fails
        return original_retrieve(query, k=k, metadata_filter=metadata_filter)

def retrieve(query: str, k=5, metadata_filter=None):
    """Main retrieval function, attempting compression if feasible."""
    if query.strip().lower().startswith("select") and " from " in query.lower():
        logger.info("Retrieve: SQL query detected. Using original_retrieve.")
        return original_retrieve(query, k, metadata_filter) # SQL queries go directly
    
    # Feature flag for compression (e.g., from env var or config)
    use_compression = bool(os.getenv("GOOGLE_API_KEY")) # Only use compression if LLM is available
    # use_compression = True # Forcing for test

    if use_compression:
        logger.info("Retrieve: Attempting retrieval with compression.")
        try:
            comp_result = retrieve_with_compression(query, metadata_filter, k)
            # Check if compression returned usable results
            if comp_result["type"] == "documents" and comp_result.get("data"):
                return comp_result
            else:
                logger.warning("Retrieve: Compression did not yield results or failed. Falling back to standard retrieval.")
        except Exception as e:
            logger.error(f"Retrieve: Error in retrieve_with_compression: {e}. Falling back.")
    else:
        logger.info("Retrieve: Compression not enabled (e.g. no API key). Using standard retrieval.")
    
    return original_retrieve(query, k, metadata_filter) # Fallback